version: '3.8'  # Использована более новая версия

services:
  llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: vm_llm_server
    ports:
      - "8080:8080"
    volumes:
      - ~/docker-share/models:/models
    environment:
      GGML_CPU_N_THREADS: 16
    command: >
      --model /models/saiga_yandexgpt_8b.Q8_0.gguf
      --port 8080
      --host 0.0.0.0
      --ctx-size 2048
      --no-mmap
    deploy:  # Исправлено: ресурсы теперь в разделе deploy
      resources:
        limits:
          memory: 16G
          cpus: '8'
    restart: unless-stopped

  vm-dashboard:
    build:
      context: ./../
      dockerfile: ./docker/Dockerfile
    image: arina/sber:master
    container_name: vm_dashboard
    ports:
      - "8050:8050"
      - "8501:8501"
    volumes:
      - ~/docker-share/data:/data:ro
    environment:
      DASH_DATA_DIR: "${DASH_DATA_DIR:-/app/data}"
      SERVER_PORT: 8501
      SERVER_HOST: "localhost"
    depends_on:
      - llama-server
    deploy:  # Исправлено: ресурсы теперь в разделе deploy
      resources:
        limits:
          memory: 2G
          cpus: '2'
    restart: unless-stopped
