from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import re


class ServerMetricsAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–µ—Ä–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ qween2.5-3b-instruct-q6
    """

    def __init__(self, use_simple=True):
        """
        use_simple: True - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, False - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —É–º–Ω—É—é
        (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å qween2.5-3b-instruct-q6)
        """
        self.use_simple = use_simple
        self.device = self._get_device()
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        self.model = None
        self.tokenizer = None

    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"

    def _load_model(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        """
        if self.model is not None:
            return

        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")

        try:
            if self.use_simple:
                # –°—Ç–∞–±–∏–ª—å–Ω–∞—è –∏ –Ω–∞–¥–µ–∂–Ω–∞—è –º–æ–¥–µ–ª—å
                model_name = "Qwen/Qwen2.5-3B-Instruct"
            else:
                # –ë–æ–ª–µ–µ —É–º–Ω–∞—è –º–æ–¥–µ–ª—å (–Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
                model_name = "Qwen/Qwen2.5-3B-Instruct"

            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="left"
            )

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            torch_dtype = torch.float16 if self.device == "cuda" else torch.float32

            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch_dtype,
                trust_remote_code=True,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            )

            if self.device == "mps":
                self.model.to("mps")

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
            self._load_fallback_model()

    def _load_fallback_model(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Å—Ç–µ–π—à—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ fallback (—Ç–æ–∂–µ —Å–∞–º–æ–µ –µ–π –±—É–¥–µ—Ç qween2.5-3b-instruct-q6)
        """
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ fallback –º–æ–¥–µ–ª–∏...")
        try:
            self.model = pipeline(
                "text-generation",
                model="Qwen/Qwen2.5-3B-Instruct",
                device=self.device,
                max_length=200
            )
            self.use_simple = True
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ fallback –º–æ–¥–µ–ª—å: Qwen/Qwen2.5-3B-Instruct")
        except:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
            self.model = None

    def parse_metrics(self, query):
        """–ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        metrics = {}

        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        percent_matches = re.findall(r'(\d+)\s*%', query)
        if percent_matches:
            metrics['percentages'] = [int(p) for p in percent_matches]

        # –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        patterns = {
            'cpu': r'(?:cpu|—Ü–ø—É|–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä)[:\s]*(\d+)%?',
            'ram': r'(?:ram|–ø–∞–º—è—Ç—å|memory|mem)[:\s]*(\d+)%?',
            'disk': r'(?:disk|–¥–∏—Å–∫)[:\s]*(\d+)%?',
            'network': r'(?:—Å–µ—Ç—å|network)[:\s]*(\d+)%?',
            'requests': r'(\d+)\s*(?:–∑–∞–ø—Ä–æ—Å–æ–≤|requests)'
        }

        for key, pattern in patterns.items():
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                try:
                    metrics[key] = int(match.group(1))
                except:
                    pass

        return metrics

    def generate_response(self, query, metrics):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫"""

        # –ï—Å–ª–∏ –Ω–µ—Ç –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        if self.model is None:
            return self._rule_based_analysis(metrics)

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
        if self.use_simple:
            prompt = self._create_simple_prompt(query, metrics)
        else:
            prompt = self._create_detailed_prompt(query, metrics)

        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            if isinstance(self.model, pipeline):
                # –î–ª—è pipeline
                response = self.model(
                    prompt,
                    max_length=500,
                    temperature=0.7,
                    do_sample=True,
                    top_p=1.1,
                    num_return_sequences=1
                )[0]['generated_text']
            else:
                # –î–ª—è –æ–±—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)

                if self.device == "mps":
                    inputs = {k: v.to("mps") for k, v in inputs.items()}
                elif self.device == "cuda":
                    inputs = {k: v.to("cuda") for k, v in inputs.items()}

                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=500,
                        temperature=0.7,
                        do_sample=True,
                        top_p=1.1,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )

                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å
            if prompt in response:
                response = response[len(prompt):].strip()

            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = self._clean_response(response)

            return response

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return self._rule_based_analysis(metrics)

    def _create_simple_prompt(self, query, metrics):
        """–ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–µ—Ç—Ä–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞, –Ω–∞–π–¥–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.

–ú–µ—Ç—Ä–∏–∫–∏: {query}

–ê–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
1. –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:
2. –ü—Ä–æ–±–ª–µ–º—ã:
3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:"""

    def _create_detailed_prompt(self, query, metrics):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —É–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        metrics_text = ""
        for key, value in metrics.items():
            if key != 'percentages':
                metrics_text += f"{key.upper()}: {value}\n"

        if metrics.get('percentages'):
            metrics_text += f"–ü—Ä–æ—Ü–µ–Ω—Ç—ã: {metrics['percentages']}\n"

        return f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–∏—Å—Ç–µ–º–Ω–æ–º—É –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—é. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞:

–ó–∞–ø—Ä–æ—Å: {query}

–ú–µ—Ç—Ä–∏–∫–∏:
{metrics_text if metrics_text else '–ù–µ—Ç —Ç–æ—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫'}

–î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:
üìä –ê–ù–ê–õ–ò–ó:
‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–´:
üõ†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
üöÄ –ü–†–ò–û–†–ò–¢–ï–¢–´:

–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º."""

    def _clean_response(self, text):
        """–û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –º—É—Å–æ—Ä–∞"""
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        text = re.sub(r'[^\w\s\d%.,!?;:()\-‚Äî\n\r–∞-—è–ê-–Ø—ë–Å]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()

        # –ò—â–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
        lines = text.split('\n')
        clean_lines = []

        for line in lines:
            line = line.strip()
            if line and len(line) > 3:
                # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º
                if not re.search(r'[^\w\s\d%.,!?;:()\-‚Äî–∞-—è–ê-–Ø—ë–Å]', line[:20]):
                    clean_lines.append(line)

        return '\n'.join(clean_lines[:20])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

    def _rule_based_analysis(self, metrics):
        """
        –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª (fallback)
        """
        analysis = []
        recommendations = []
        priorities = []

        # –ê–Ω–∞–ª–∏–∑ CPU
        cpu = metrics.get('cpu')
        if cpu:
            if cpu > 90:
                analysis.append("üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU")
                recommendations.append("–°—Ä–æ—á–Ω–æ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å—ã, –Ω–∞–π–¥–∏—Ç–µ —É—Ç–µ—á–∫–∏")
                priorities.append("1. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, —É–±–∏—Ç—å —Ç—è–∂–µ–ª—ã–µ")
            elif cpu > 70:
                analysis.append("üü° –í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU")
                recommendations.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–¥, –¥–æ–±–∞–≤—å—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ")
                priorities.append("1. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤")
            else:
                analysis.append("‚úÖ CPU –≤ –Ω–æ—Ä–º–µ")

        # –ê–Ω–∞–ª–∏–∑ RAM
        ram = metrics.get('ram')
        if ram:
            if ram > 90:
                analysis.append("üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏")
                recommendations.append("–£–≤–µ–ª–∏—á—å—Ç–µ RAM –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ swap")
                priorities.append("2. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏")
            elif ram > 75:
                analysis.append("üü° –í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏")
                recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏")
                priorities.append("2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏")
            else:
                analysis.append("‚úÖ –ü–∞–º—è—Ç—å –≤ –Ω–æ—Ä–º–µ")

        # –ê–Ω–∞–ª–∏–∑ Disk
        disk = metrics.get('disk')
        if disk:
            if disk > 95:
                analysis.append("üî¥ –î–∏—Å–∫ –ø–æ—á—Ç–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω")
                recommendations.append("–°—Ä–æ—á–Ω–æ –æ—á–∏—Å—Ç–∏—Ç–µ –º–µ—Å—Ç–æ")
                priorities.append("3. –û—á–∏—Å—Ç–∫–∞ –¥–∏—Å–∫–∞")
            elif disk > 80:
                analysis.append("üü° –ú–∞–ª–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ")
                recommendations.append("–£–¥–∞–ª–∏—Ç–µ —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã")
                priorities.append("3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–µ–Ω–∏—è")
            else:
                analysis.append("‚úÖ –î–∏—Å–∫ –≤ –Ω–æ—Ä–º–µ")

        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –¥–∞–µ–º –æ–±—â–∏–π —Å–æ–≤–µ—Ç
        if not analysis:
            percentages = metrics.get('percentages', [])
            if percentages:
                avg_percent = sum(percentages) / len(percentages)
                if avg_percent > 80:
                    analysis.append("‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –æ–±—â–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞")
                    recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π –∞—É–¥–∏—Ç —Å–∏—Å—Ç–µ–º—ã")
                else:
                    analysis.append("‚úÖ –ù–∞–≥—Ä—É–∑–∫–∞ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
            else:
                analysis.append("‚ÑπÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ç–æ—á–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏")
                recommendations.append("–£—Ç–æ—á–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è CPU, RAM, Disk –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        result = []
        if analysis:
            result.append("üìä –ê–ù–ê–õ–ò–ó:")
            result.extend([f"- {a}" for a in analysis])

        if recommendations:
            result.append("\nüõ†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            result.extend([f"- {r}" for r in recommendations])

        if priorities:
            result.append("\nüöÄ –ü–†–ò–û–†–ò–¢–ï–¢–´:")
            result.extend([f"{p}" for p in priorities])

        return '\n'.join(result) if result else "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏"

    def analyze(self, query):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞"""
        print(f"üìã –ó–∞–ø—Ä–æ—Å: {query}")

        # –ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = self.parse_metrics(query)
        print(f"üìä –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {metrics}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self._load_model()

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        return self.generate_response(query, metrics)


# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def analyze_server_metrics(query, use_simple=True):
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–µ—Ä–∞

    Args:
        query: —Å—Ç—Ä–æ–∫–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–Ω–∞–ø—Ä., "CPU 85%, RAM 70%")
        use_simple: True –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, False –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–π

    Returns:
        –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞
    """
    analyzer = ServerMetricsAnalyzer(use_simple=use_simple)
    return analyzer.analyze(query)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üîç –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–µ—Ä–∞")
    print("=" * 50)

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "CPU: 92%, RAM: 88%, Disk: 45%",
        "–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä 45%, –ø–∞–º—è—Ç—å 60%, –¥–∏—Å–∫ 98%",
        "–°–µ—Ç—å 75%, CPU 60%, 250 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É",
        "–°–µ—Ä–≤–µ—Ä –ª–∞–≥–∞–µ—Ç, –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–∞ –º–∞–∫—Å–∏–º—É–º–µ",
        "–í—Å–µ –≤ –Ω–æ—Ä–º–µ, CPU 30% RAM 40%",
        "–ü–∞–º—è—Ç—å 95%, –¥–∏—Å–∫ 85%"
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"\n{'=' * 40}")
        print(f"–¢–ï–°–¢ #{i}: {query}")
        print('=' * 40)

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            result = analyze_server_metrics(query, use_simple=True)
            print(f"\nüìù –†–ï–ó–£–õ–¨–¢–ê–¢:\n{result}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

    print(f"\n{'=' * 50}")
    print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print("\nüéØ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (–¥–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥')")
    print("=" * 50)

    while True:
        user_input = input("\nüì• –í–≤–µ–¥–∏—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: ").strip()

        if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', '']:
            break

        if user_input:
            try:
                result = analyze_server_metrics(user_input, use_simple=True)
                print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê:\n{result}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
                print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–≤–µ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: CPU 85%, RAM 70%, Disk 60%")
