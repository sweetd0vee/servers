"""
–ï–¥–∏–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã:
- hf_api: Hugging Face Inference API (–≤–Ω–µ—à–Ω–∏–π —Å–µ—Ä–≤–∏—Å)
- local: –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ transformers (Qwen/Qwen2.5-3B-Instruct)
- rule_based: –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª (fallback)
- auto: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Å fallback —Ü–µ–ø–æ—á–∫–æ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –ü—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞):
    from app.llm import call_ai_analysis

    context = {
        'servers': {
            'server1': {'cpu_avg': 85, 'mem_avg': 70, 'has_anomalies': True}
        }
    }
    analysis = call_ai_analysis(context)

2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –Ω–∞–ø—Ä—è–º—É—é:
    from app.llm import ServerMetricsAnalyzer

    analyzer = ServerMetricsAnalyzer(provider="local")
    result = analyzer.analyze(context)

    # –ò–ª–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞:
    result = analyzer.analyze_query("CPU: 85%, RAM: 70%")

3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
    export LLM_PROVIDER=local
    export LLM_MODEL_NAME=Qwen/Qwen2.5-3B-Instruct
    export HF_API_KEY=your_api_key_here

4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ singleton:
    from app.llm import get_analyzer

    analyzer = get_analyzer()  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ env
    result = analyzer.analyze(context)
"""
import os
import re
import requests
from typing import Dict, Any, Optional, Union
import logging

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å streamlit (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤–Ω–µ Streamlit –æ–∫—Ä—É–∂–µ–Ω–∏—è)
try:
    import streamlit as st

    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logger = logging.getLogger(__name__)

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å transformers (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    import torch

    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")


class ServerMetricsAnalyzer:
    """
    –ï–¥–∏–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–µ—Ç—Ä–∏–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM.

    Args:
        provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –í–∞—Ä–∏–∞–Ω—Ç—ã:
            - "hf_api": Hugging Face Inference API
            - "local": –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ transformers
            - "rule_based": –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª
            - "auto": –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Å fallback (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Qwen/Qwen2.5-3B-Instruct)
        hf_api_key: API –∫–ª—é—á –¥–ª—è Hugging Face (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±–µ—Ä–µ—Ç—Å—è –∏–∑ env)
    """

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    DEFAULT_MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"
    DEFAULT_TIMEOUT = 90
    DEFAULT_MAX_TOKENS = 400

    # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Hugging Face –¥–ª—è –ø–æ–ø—ã—Ç–æ–∫ (–æ—Ç –ª–µ–≥–∫–∏—Ö –∫ —Ç—è–∂–µ–ª—ã–º)
    HF_MODELS = [
        {
            "url": "https://api-inference.huggingface.co/models/sshleifer/tiny-gpt2",
            "name": "TinyGPT2",
            "tokens": 300
        },
        {
            "url": "https://api-inference.huggingface.co/models/google/flan-t5-small",
            "name": "Flan-T5-Small",
            "tokens": 400
        },
        {
            "url": "https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-125m",
            "name": "GPT-Neo-125M",
            "tokens": 500
        },
        {
            "url": "https://api-inference.huggingface.co/models/distilgpt2",
            "name": "DistilGPT2",
            "tokens": 500
        },
        {
            "url": "https://api-inference.huggingface.co/models/microsoft/phi-2",
            "name": "Phi-2",
            "tokens": 700
        }
    ]

    def __init__(
            self,
            provider: str = "auto",
            model_name: Optional[str] = None,
            hf_api_key: Optional[str] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞.

        Args:
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ("auto", "hf_api", "local", "rule_based")
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            hf_api_key: API –∫–ª—é—á –¥–ª—è Hugging Face
        """
        self.provider = provider.lower()
        self.model_name = model_name or self.DEFAULT_MODEL_NAME
        self.hf_api_key = hf_api_key or os.getenv("HF_API_KEY")

        # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        self.model = None
        self.tokenizer = None
        self.device = None

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        if TRANSFORMERS_AVAILABLE:
            self.device = self._get_device()
            logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        self._check_provider_availability()

    def _get_device(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–µ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        if not TRANSFORMERS_AVAILABLE:
            return "cpu"

        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"

    def _check_provider_availability(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
        if self.provider == "local" and not TRANSFORMERS_AVAILABLE:
            logger.warning("–õ–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω), –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ rule_based")
            self.provider = "rule_based"

        if self.provider == "hf_api" and not self.hf_api_key:
            logger.warning("HF API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ rule_based")
            self.provider = "rule_based"

    def analyze(self, context: Union[Dict[str, Any], str]) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç—Ä–∏–∫.

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ú–æ–∂–µ—Ç –±—ã—Ç—å:
                - Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å–µ—Ä–≤–µ—Ä–æ–≤ (–∫–∞–∫ –≤ anomalies.py)
                - str —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º

        Returns:
            –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç—Ä–∏–∫
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if isinstance(context, str):
            return self.analyze_query(context)

        # –î–ª—è dict –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if self.provider == "auto":
            return self._analyze_with_fallback(context)
        elif self.provider == "hf_api":
            return self._analyze_hf_api(context)
        elif self.provider == "local":
            return self._analyze_local(context)
        elif self.provider == "rule_based":
            return self._analyze_rule_based(context)
        else:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.provider}, –∏—Å–ø–æ–ª—å–∑—É–µ–º rule_based")
            return self._analyze_rule_based(context)

    def analyze_query(self, query: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

        Returns:
            –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        # –ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        metrics = self._parse_metrics_from_query(query)

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –º–µ—Ç—Ä–∏–∫
        context = {
            'query': query,
            'metrics': metrics
        }

        return self.analyze(context)

    def _analyze_with_fallback(self, context: Dict[str, Any]) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.

        –ü–æ—Ä—è–¥–æ–∫ –ø–æ–ø—ã—Ç–æ–∫:
        1. Hugging Face API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∫–ª—é—á)
        2. –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ transformers –¥–æ—Å—Ç—É–ø–µ–Ω)
        3. Rule-based –∞–Ω–∞–ª–∏–∑ (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω)
        """
        # –ü–æ–ø—ã—Ç–∫–∞ 1: Hugging Face API
        if self.hf_api_key:
            try:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ Hugging Face API")
                result = self._analyze_hf_api(context)
                if result and len(result) > 50:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                    return result
            except Exception as e:
                logger.warning(f"HF API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        # –ü–æ–ø—ã—Ç–∫–∞ 2: –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
        if TRANSFORMERS_AVAILABLE:
            try:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å")
                result = self._analyze_local(context)
                if result and len(result) > 50:
                    return result
            except Exception as e:
                logger.warning(f"–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")

        # Fallback: Rule-based –∞–Ω–∞–ª–∏–∑
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è rule-based –∞–Ω–∞–ª–∏–∑")
        return self._analyze_rule_based(context)

    def _analyze_hf_api(self, context: Dict[str, Any]) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Hugging Face Inference API.

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

        Returns:
            –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        if not self.hf_api_key:
            raise ValueError("HF API –∫–ª—é—á –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        prompt = self._prepare_prompt_from_context(context)

        headers = {
            "Authorization": f"Bearer {self.hf_api_key}",
            "Content-Type": "application/json"
        }

        # –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É
        for model_config in self.HF_MODELS:
            try:
                data = {
                    "inputs": prompt[:800],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                    "parameters": {
                        "max_new_tokens": model_config["tokens"],
                        "temperature": 0.3,
                        "return_full_text": False
                    }
                }

                response = requests.post(
                    model_config["url"],
                    headers=headers,
                    json=data,
                    timeout=self.DEFAULT_TIMEOUT
                )

                if response.status_code == 200:
                    result = response.json()
                    analysis = self._extract_text_from_hf_response(result)
                    if analysis:
                        return f"–ê–Ω–∞–ª–∏–∑ (–º–æ–¥–µ–ª—å: {model_config['name']}):\n\n{analysis}"

            except Exception as e:
                logger.debug(f"–ú–æ–¥–µ–ª—å {model_config['name']} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
                continue

        raise Exception("–í—Å–µ –º–æ–¥–µ–ª–∏ Hugging Face –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

    def _analyze_local(self, context: Dict[str, Any]) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å transformers.

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

        Returns:
            –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        if self.model is None:
            self._load_local_model()

        if self.model is None:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å")

        prompt = self._prepare_prompt_from_context(context)

        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            if isinstance(self.model, pipeline):
                # Pipeline –ø–æ–¥—Ö–æ–¥
                response = self.model(
                    prompt,
                    max_length=500,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    num_return_sequences=1
                )[0]['generated_text']
            else:
                # –ü—Ä—è–º–∞—è —Ä–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—å—é
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512
                )

                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                if self.device in ["cuda", "mps"]:
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}

                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=self.DEFAULT_MAX_TOKENS,
                        temperature=0.7,
                        do_sample=True,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )

                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
            if prompt in response:
                response = response[len(prompt):].strip()

            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = self._clean_response(response)

            return response

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            raise

    def _load_local_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å transformers."""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {self.model_name}")

            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="left"
            )

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º dtype
            torch_dtype = torch.float16 if self.device == "cuda" else torch.float32

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch_dtype,
                trust_remote_code=True,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            )

            # –î–ª—è MPS –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Ä—É—á–Ω—É—é
            if self.device == "mps":
                self.model.to("mps")

            logger.info(f"–ú–æ–¥–µ–ª—å {self.model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            # –ü—Ä–æ–±—É–µ–º fallback —á–µ—Ä–µ–∑ pipeline
            try:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ pipeline")
                self.model = pipeline(
                    "text-generation",
                    model=self.model_name,
                    device=self.device if self.device != "mps" else -1,
                    max_length=200
                )
                logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ pipeline")
            except Exception as e2:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e2}")
                self.model = None

    def _analyze_rule_based(self, context: Dict[str, Any]) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª (fallback).

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

        Returns:
            –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        metrics = self._extract_metrics_from_context(context)

        analysis = []
        recommendations = []
        priorities = []

        # –ê–Ω–∞–ª–∏–∑ CPU
        cpu = metrics.get('cpu')
        if cpu is not None:
            if cpu > 90:
                analysis.append("üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU")
                recommendations.append("–°—Ä–æ—á–Ω–æ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å—ã, –Ω–∞–π–¥–∏—Ç–µ —É—Ç–µ—á–∫–∏")
                priorities.append("1. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, —É–±–∏—Ç—å —Ç—è–∂–µ–ª—ã–µ")
            elif cpu > 70:
                analysis.append("üü° –í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU")
                recommendations.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –∫–æ–¥, –¥–æ–±–∞–≤—å—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ")
                priorities.append("1. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤")
            else:
                analysis.append("‚úÖ CPU –≤ –Ω–æ—Ä–º–µ")

        # –ê–Ω–∞–ª–∏–∑ RAM
        ram = metrics.get('ram') or metrics.get('mem')
        if ram is not None:
            if ram > 90:
                analysis.append("üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏")
                recommendations.append("–£–≤–µ–ª–∏—á—å—Ç–µ RAM –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ swap")
                priorities.append("2. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏")
            elif ram > 75:
                analysis.append("üü° –í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏")
                recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏")
                priorities.append("2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏")
            else:
                analysis.append("‚úÖ –ü–∞–º—è—Ç—å –≤ –Ω–æ—Ä–º–µ")

        # –ê–Ω–∞–ª–∏–∑ Disk
        disk = metrics.get('disk')
        if disk is not None:
            if disk > 95:
                analysis.append("üî¥ –î–∏—Å–∫ –ø–æ—á—Ç–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω")
                recommendations.append("–°—Ä–æ—á–Ω–æ –æ—á–∏—Å—Ç–∏—Ç–µ –º–µ—Å—Ç–æ")
                priorities.append("3. –û—á–∏—Å—Ç–∫–∞ –¥–∏—Å–∫–∞")
            elif disk > 80:
                analysis.append("üü° –ú–∞–ª–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ")
                recommendations.append("–£–¥–∞–ª–∏—Ç–µ —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã")
                priorities.append("3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–µ–Ω–∏—è")
            else:
                analysis.append("‚úÖ –î–∏—Å–∫ –≤ –Ω–æ—Ä–º–µ")

        # –ê–Ω–∞–ª–∏–∑ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if 'servers' in context:
            servers = context['servers']
            for server_name, server_data in servers.items():
                if server_data.get('has_anomalies'):
                    cpu_avg = server_data.get('cpu_avg', 0)
                    mem_avg = server_data.get('mem_avg', 0)
                    analysis.append(f"‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä {server_name}: CPU={cpu_avg}%, RAM={mem_avg}%")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏
        if 'statistical_anomalies' in context and context['statistical_anomalies']:
            anomalies_count = len(context['statistical_anomalies'])
            analysis.append(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {anomalies_count} —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∞–Ω–æ–º–∞–ª–∏–π")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–µ—Ä–≤–µ—Ä—ã —Å –∞–Ω–æ–º–∞–ª–∏—è–º–∏ –≤ –¥–µ—Ç–∞–ª—è—Ö")

        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –¥–∞–µ–º –æ–±—â–∏–π —Å–æ–≤–µ—Ç
        if not analysis:
            percentages = metrics.get('percentages', [])
            if percentages:
                avg_percent = sum(percentages) / len(percentages)
                if avg_percent > 80:
                    analysis.append("‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –æ–±—â–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞")
                    recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π –∞—É–¥–∏—Ç —Å–∏—Å—Ç–µ–º—ã")
                else:
                    analysis.append("‚úÖ –ù–∞–≥—Ä—É–∑–∫–∞ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
            else:
                analysis.append("‚ÑπÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ç–æ—á–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏")
                recommendations.append("–£—Ç–æ—á–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è CPU, RAM, Disk –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        result = []
        if analysis:
            result.append("üìä **–ê–ù–ê–õ–ò–ó:**")
            result.extend([f"- {a}" for a in analysis])

        if recommendations:
            result.append("\nüõ†Ô∏è **–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:**")
            result.extend([f"- {r}" for r in recommendations])

        if priorities:
            result.append("\nüöÄ **–ü–†–ò–û–†–ò–¢–ï–¢–´:**")
            result.extend([f"{p}" for p in priorities])

        return '\n'.join(result) if result else "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏"

    def _prepare_prompt_from_context(self, context: Dict[str, Any]) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM.

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

        Returns:
            –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
        """
        # –ï—Å–ª–∏ –µ—Å—Ç—å query, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if 'query' in context:
            query = context['query']
            return f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–∏—Å—Ç–µ–º–Ω–æ–º—É –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—é. 
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–µ—Ç—Ä–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞ –∏ –¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.

–ó–∞–ø—Ä–æ—Å: {query}

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
1. –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
2. –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
4. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π

–ê–Ω–∞–ª–∏–∑:"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        prompt_parts = ["–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–∏—Å—Ç–µ–º–Ω–æ–º—É –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—é. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–µ—Ç—Ä–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–æ–≤:\n"]

        if 'servers' in context:
            prompt_parts.append("–ú–µ—Ç—Ä–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–æ–≤:")
            for server_name, server_data in context['servers'].items():
                cpu_avg = server_data.get('cpu_avg', 0)
                mem_avg = server_data.get('mem_avg', 0)
                mem_max = server_data.get('mem_max', 0)
                cpu_max = server_data.get('cpu_max', 0)
                prompt_parts.append(
                    f"- {server_name}: CPU —Å—Ä–µ–¥–Ω–µ–µ={cpu_avg}%, –º–∞–∫—Å={cpu_max}%, "
                    f"RAM —Å—Ä–µ–¥–Ω–µ–µ={mem_avg}%, –º–∞–∫—Å={mem_max}%"
                )

        if 'statistical_anomalies' in context and context['statistical_anomalies']:
            prompt_parts.append("\n–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏:")
            for anomaly in context['statistical_anomalies'][:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
                prompt_parts.append(
                    f"- {anomaly['server']} ({anomaly['date']}): "
                    f"{anomaly['metric']}={anomaly['value']:.2f}% "
                    f"(Z-score: {anomaly['z_score']:.2f})"
                )

        prompt_parts.append("\n–î–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:")
        prompt_parts.append("üìä –ê–ù–ê–õ–ò–ó:")
        prompt_parts.append("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–´:")
        prompt_parts.append("üõ†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        prompt_parts.append("üöÄ –ü–†–ò–û–†–ò–¢–ï–¢–´:")
        prompt_parts.append("\n–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º.")

        return "\n".join(prompt_parts)

    def _extract_metrics_from_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è rule-based –∞–Ω–∞–ª–∏–∑–∞.

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        metrics = {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ servers
        if 'servers' in context:
            servers = context['servers']
            cpu_values = []
            ram_values = []

            for server_data in servers.values():
                if 'cpu_avg' in server_data:
                    cpu_values.append(server_data['cpu_avg'])
                if 'mem_avg' in server_data:
                    ram_values.append(server_data['mem_avg'])

            if cpu_values:
                metrics['cpu'] = sum(cpu_values) / len(cpu_values)
            if ram_values:
                metrics['ram'] = sum(ram_values) / len(ram_values)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ query –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'query' in context:
            parsed = self._parse_metrics_from_query(context['query'])
            metrics.update(parsed)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ metrics –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'metrics' in context:
            metrics.update(context['metrics'])

        return metrics

    def _parse_metrics_from_query(self, query: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        metrics = {}

        # –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        percent_matches = re.findall(r'(\d+)\s*%', query)
        if percent_matches:
            metrics['percentages'] = [int(p) for p in percent_matches]

        # –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        patterns = {
            'cpu': r'(?:cpu|—Ü–ø—É|–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä)[:\s]*(\d+)%?',
            'ram': r'(?:ram|–ø–∞–º—è—Ç—å|memory|mem)[:\s]*(\d+)%?',
            'disk': r'(?:disk|–¥–∏—Å–∫)[:\s]*(\d+)%?',
            'network': r'(?:—Å–µ—Ç—å|network)[:\s]*(\d+)%?',
            'requests': r'(\d+)\s*(?:–∑–∞–ø—Ä–æ—Å–æ–≤|requests)'
        }

        for key, pattern in patterns.items():
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                try:
                    metrics[key] = int(match.group(1))
                except:
                    pass

        return metrics

    def _extract_text_from_hf_response(self, data: Any) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ Hugging Face API.

        Args:
            data: –û—Ç–≤–µ—Ç –æ—Ç API

        Returns:
            –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        if isinstance(data, str):
            return data
        elif isinstance(data, dict):
            for key in ['generated_text', 'text', 'output', 'response']:
                if key in data:
                    return str(data[key])
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–ª—é—á, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π –Ω–µ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∫–ª—é—á
            for key in data:
                if key not in ['error', 'warnings', 'status']:
                    return str(data[key])
            return str(data)
        elif isinstance(data, list) and len(data) > 0:
            return self._extract_text_from_hf_response(data[0])
        return str(data)

    def _clean_response(self, text: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –æ—Ç –º—É—Å–æ—Ä–∞.

        Args:
            text: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞

        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        text = re.sub(r'[^\w\s\d%.,!?;:()\-‚Äî\n\r–∞-—è–ê-–Ø—ë–Å]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()

        # –ò—â–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
        lines = text.split('\n')
        clean_lines = []

        for line in lines:
            line = line.strip()
            if line and len(line) > 3:
                # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º
                if not re.search(r'[^\w\s\d%.,!?;:()\-‚Äî–∞-—è–ê-–Ø—ë–Å]', line[:20]):
                    clean_lines.append(line)

        return '\n'.join(clean_lines[:20])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
_global_analyzer = None


def get_analyzer(provider: str = None) -> ServerMetricsAnalyzer:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ (singleton pattern).

    Args:
        provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ env –∏–ª–∏ "auto")

    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä ServerMetricsAnalyzer
    """
    global _global_analyzer

    if _global_analyzer is None or (provider and _global_analyzer.provider != provider):
        if provider is None:
            provider = os.getenv("LLM_PROVIDER", "auto")

        _global_analyzer = ServerMetricsAnalyzer(provider=provider)

    return _global_analyzer


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def call_ai_analysis(context: Union[Dict[str, Any], str]) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º.

    Args:
        context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
    """
    analyzer = get_analyzer()
    return analyzer.analyze(context)


def analyze_server_metrics(query: str, use_simple: bool = True) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º.

    Args:
        query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        use_simple: –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)

    Returns:
        –¢–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞
    """
    analyzer = get_analyzer()
    return analyzer.analyze_query(query)


def local_ai_analysis(context):
    """–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ API"""
    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    analysis_result = """**–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:**
–ü—Ä–æ–≤–µ–¥–µ–Ω –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫. –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ AI API.

‚ö†Ô∏è **–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä—ã:**
–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–µ—Ä–≤–µ—Ä—ã —Å –ø–∏–∫–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ CPU > 80% –∏ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç—å—é < 20%.

üéØ **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
1. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–æ–π
2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–∞—Ö —Å –∞–Ω–æ–º–∞–ª–∏—è–º–∏
3. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""

    return analysis_result
